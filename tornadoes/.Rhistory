rep_f = int(len(df_zero) / len(df_g))
# oversample of death
df_gg = pd.concat([df_g] * rep_f, ignore_index=True)
# combining dfs again
frames = [df_zero, df_gg]
df = pd.concat(frames, ignore_index=True)
######## Classification ########################################################
print("Classifying... \n")
df.loc[:, 'has_fat'] = df['fat'].apply(transform_value)
df['has_fat'] = df['has_fat'].astype('category')
X = df.drop(columns = ['has_fat', 'fat'])
y = df.has_fat
X_train, X_test, y_train, y_test = time_based_train_test_split(df.drop(columns = ['fat']), perc=0.8, pred_col = "has_fat")
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# Save for later... save for regression testing
train_rm, X_test_regr, y_train_rm, y_test_regr = time_based_train_test_split(df.drop(columns = ['has_fat']), perc=0.8, pred_col = "fat")
X_train
X_val
X_test
X_test_regr['class'] = y_test_pred
best_classifier
y_train_pred = best_classifier.predict(X_train)
mcc_train = matthews_corrcoef(y_train, y_train_pred).round(3)
acc_train = accuracy_score(y_train, y_train_pred).round(3)
# Calculate MCC for the validation set
y_val_pred = best_classifier.predict(X_val)
mcc_val = matthews_corrcoef(y_val, y_val_pred).round(3)
acc_val = accuracy_score(y_val, y_val_pred).round(3)
# Calculate MCC for the test set
y_test_pred = best_classifier.predict(X_test)
mcc_test = matthews_corrcoef(y_test, y_test_pred).round(3)
acc_test = accuracy_score(y_test, y_test_pred).round(3)
print(f"Training MCC: {mcc_train}")
print(f"Validation MCC: {mcc_val}")
print(f"Test MCC: {mcc_test} \n")
print(f"Training Accuracy: {acc_train}")
print(f"Validation Accuracy: {acc_val}")
print(f"Test Accuracy: {acc_test} \n")
# Confusion Matrix
# Training Confusion Matrix
print("Training Confusion Matrix \n")
conf_matrix = confusion_matrix(y_train, y_train_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])
print(conf_matrix_df)
# Validation Confusion Matrix
print("Validation Confusion Matrix \n")
conf_matrix = confusion_matrix(y_val, y_val_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])
print(conf_matrix_df)
# Testing Confusion Matrix
print("Testing Confusion Matrix \n")
conf_matrix = confusion_matrix(y_test, y_test_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])
print(conf_matrix_df)
# Classification Feature Importance
try:
coefficients = best_classifier.coef_[0]
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance = feature_importance.sort_values('Importance', ascending=False)
feature_importance.plot.bar(x='Feature', y='Importance', figsize=(10, 6))
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.savefig('output/classifier-feat-importance.png')
plt.show()
except:
coefficients = best_classifier.feature_importances_
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance = feature_importance.sort_values('Importance', ascending=False)
feature_importance.plot.bar(x='Feature', y='Importance', figsize=(12, 12))
plt.xticks(rotation=45, ha='right')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.savefig('output/classifier-feat-importance.png')
plt.show()
X_test_regr['class'] = y_test_pred
X_test_regr['fat'] = y_test_regr
X_test_final = X_test_regr[X_test_regr['class'] == '1'].drop(columns = ['class'])
# X_test_final = X_test_regr[X_test_regr['fat'] > 0].drop(columns = ['class'])
X_test_final = X_test_final.drop_duplicates()
# df_g = df_g[['yr', 'mo', 'dy', 'fat', 'traveled_d_km','area_sq_km', 'day_of_week']].drop_duplicates()
df_g = df_g[['yr', 'mo', 'dy', 'fat', 'traveled_d_km','area_sq_km', 'day_of_week']]
X_train, X_test, y_train, y_test = time_based_train_test_split(df = df_g, perc=0.8, year_col="yr", pred_col='fat')
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# Overwrite test dataset with the one classifed
X_test = X_test_final.drop(columns = ['fat'])
# y_test = np.log1p(X_test_final['fat'])
y_test = X_test_final['fat']
X_train
X_val
X_test
y_test_pred
X_test_regr
df_g
df_g
X_test_regr
X_test_final = X_test_regr[X_test_regr['fat'] > 0].drop(columns = ['class'])
X_test_final
X_test_final.drop_duplicates()
X_test_final = X_test_regr[X_test_regr['fat'] > 0].drop_dupicates()
X_test_final = X_test_regr[X_test_regr['fat'] > 0]
X_test_final
X_tets
X_test
X_train, X_test, y_train, y_test = time_based_train_test_split(df.drop(columns = ['fat']), perc=0.8, pred_col = "has_fat")
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# Save for later... save for regression testing
train_rm, X_test_regr, y_train_rm, y_test_regr = time_based_train_test_split(df.drop(columns = ['has_fat']), perc=0.8, pred_col = "fat")
X_test
X_test['has_fat'] = y_test
X_test
X_test.drop_duplicates()
X_test = X_test.drop_duplicates()
y_test = X_test['has_fat']
best_classifier.predict(y_test)
X_test = X_test.drop(columns=['has_fat'])
X_test
ypred = best_classifier.predict(X_test)
accuracy_score(ypred, y_test)
X_test
X = df.drop(columns = ['has_fat', 'fat'])
y = df.has_fat
X_train, X_test, y_train, y_test = time_based_train_test_split(df.drop(columns = ['fat']), perc=0.8, pred_col = "has_fat")
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# Save for later... save for regression testing
train_rm, X_test_regr, y_train_rm, y_test_regr = time_based_train_test_split(df.drop(columns = ['has_fat']), perc=0.8, pred_col = "fat")
X_train
X_train.yr
X_train.yr.max()
df_zero.yr
plt.histogram(df_zero.yr)
plt.hist(df_zero.yr)
plt.show()
plt.hist(df_g.yr)
plt.show()
data_file = 'input/us_tornado_dataset_1950_2021.csv'
# Data wrangling ###############################################################
print("Wrangling the data... \n")
df = pd.read_csv(data_file)
# df.loc[df.mag==-9, 'mag'] = 0 #unknown magnitude when -9, so 0
df.loc[df.elat == 0.0, 'elat'] = df['slat'] #unknown final lat/long --> = to initial ones
df.loc[df.elon == 0.0, 'elon'] = df['slon']
df['traveled_d_km'] = df.apply(calculate_distance, axis=1)
df['len_km'] = df['len'].apply(miles_to_kilometers)
df['wid_km'] = df['wid'].apply(yards_to_kilometers)
df['area_sq_km'] = df.apply(calculate_tornado_area, axis=1)
# Format 'mo' and 'dy' columns with leading zeros
df['mo'] = df['mo'].apply(lambda x: str(x).zfill(2))
df['dy'] = df['dy'].apply(lambda x: str(x).zfill(2))
# Create a new 'date' column in the "year-month-day" format
df['date'] = pd.to_datetime(df[['yr', 'mo', 'dy']].astype(str).agg('-'.join, axis=1), format='%Y-%m-%d')
df['yr'] = df['yr'].astype(int)
df['mo'] = df['mo'].astype(int)
df['dy'] = df['dy'].astype(int)
# Extract the day of the week and create a new 'day_of_week' column
df['day_of_week'] = df['date'].dt.dayofweek
# Keeping only relevant columns
df = df[['yr', 'mo', 'dy', 'fat', 'traveled_d_km','area_sq_km', 'day_of_week']]
# Outlier Removal
df = df[df['fat'] < 30] # histogram based
df = df[df['traveled_d_km'] < 50] # histogram based
df = df[df['area_sq_km'] < 20] # histogram based
# Filtering death
df_zero = df[df.fat==0]
# Filtering death
df_g = df[df.fat>0]
# replication factor to balance death
rep_f = int(len(df_zero) / len(df_g))
# oversample of death
df_gg = pd.concat([df_g] * rep_f, ignore_index=True)
# combining dfs again
frames = [df_zero, df_gg]
df = pd.concat(frames, ignore_index=True)
######## Classification ########################################################
print("Classifying... \n")
df.loc[:, 'has_fat'] = df['fat'].apply(transform_value)
df['has_fat'] = df['has_fat'].astype('category')
X = df.drop(columns = ['has_fat', 'fat'])
y = df.has_fat
X_train, X_test, y_train, y_test = time_based_train_test_split(df.drop(columns = ['fat']), perc=0.8, pred_col = "has_fat")
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# Save for later... save for regression testing
train_rm, X_test_regr, y_train_rm, y_test_regr = time_based_train_test_split(df.drop(columns = ['has_fat']), perc=0.8, pred_col = "fat")
# Model Training and/or hyperparameter tuning ##################################
# Check if there's an available model
model_file = 'model/best_model_class.pkl'
try:
with open(model_file, 'rb') as file:
best_classifier = pickle.load(file)
except:
best_classifier = tune_classifier(X_train, y_train, X_val, y_val, model_file = model_file)
print(best_classifier)
# Calculate MCC for the training set
y_train_pred = best_classifier.predict(X_train)
mcc_train = matthews_corrcoef(y_train, y_train_pred).round(3)
acc_train = accuracy_score(y_train, y_train_pred).round(3)
# Calculate MCC for the validation set
y_val_pred = best_classifier.predict(X_val)
mcc_val = matthews_corrcoef(y_val, y_val_pred).round(3)
acc_val = accuracy_score(y_val, y_val_pred).round(3)
# Calculate MCC for the test set
y_test_pred = best_classifier.predict(X_test)
mcc_test = matthews_corrcoef(y_test, y_test_pred).round(3)
acc_test = accuracy_score(y_test, y_test_pred).round(3)
print(f"Training MCC: {mcc_train}")
print(f"Validation MCC: {mcc_val}")
print(f"Test MCC: {mcc_test} \n")
print(f"Training Accuracy: {acc_train}")
print(f"Validation Accuracy: {acc_val}")
print(f"Test Accuracy: {acc_test} \n")
# Confusion Matrix
# Training Confusion Matrix
print("Training Confusion Matrix \n")
conf_matrix = confusion_matrix(y_train, y_train_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])
print(conf_matrix_df)
# Validation Confusion Matrix
print("Validation Confusion Matrix \n")
conf_matrix = confusion_matrix(y_val, y_val_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])
print(conf_matrix_df)
# Testing Confusion Matrix
print("Testing Confusion Matrix \n")
conf_matrix = confusion_matrix(y_test, y_test_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])
print(conf_matrix_df)
# Classification Feature Importance
try:
coefficients = best_classifier.coef_[0]
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance = feature_importance.sort_values('Importance', ascending=False)
feature_importance.plot.bar(x='Feature', y='Importance', figsize=(10, 6))
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.savefig('output/classifier-feat-importance.png')
plt.show()
except:
coefficients = best_classifier.feature_importances_
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance = feature_importance.sort_values('Importance', ascending=False)
feature_importance.plot.bar(x='Feature', y='Importance', figsize=(12, 12))
plt.xticks(rotation=45, ha='right')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.savefig('output/classifier-feat-importance.png')
plt.show()
####### Regression #############################################################
print("Regression... \n")
# Use the classified tornadoes as test for regression
X_test_regr['class'] = y_test_pred
X_test_regr['fat'] = y_test_regr
X_test_final = X_test_regr[X_test_regr['class'] == '1'].drop(columns = ['class'])
# X_test_final = X_test_regr[X_test_regr['fat'] > 0].drop(columns = ['class'])
X_test_final = X_test_final.drop_duplicates()
# df_g = df_g[['yr', 'mo', 'dy', 'fat', 'traveled_d_km','area_sq_km', 'day_of_week']].drop_duplicates()
df_g = df_g[['yr', 'mo', 'dy', 'fat', 'traveled_d_km','area_sq_km', 'day_of_week']]
# df_g.loc[:, 'fat'] = np.log1p(df_g['fat'])
plt.hist(df_g.fat, bins=30, edgecolor='k')  # You can adjust the number of bins as needed
plt.show()
################################################################################
# df_g['fat'] = np.log1p(df_g['fat'])
################################################################################
# dft = df_g.groupby('yr')['fat'].sum().reset_index().drop_duplicates()
# plt.bar(dft['yr'], dft['fat'], color='blue', alpha=0.7)
# plt.xlabel('Year')
# plt.ylabel('Fatalities')
# plt.title('Fatalities Over Time')
# plt.show()
X_train, X_test, y_train, y_test = time_based_train_test_split(df = df_g, perc=0.8, year_col="yr", pred_col='fat')
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# Overwrite test dataset with the one classifed
X_test = X_test_final.drop(columns = ['fat'])
# y_test = np.log1p(X_test_final['fat'])
y_test = X_test_final['fat']
plt.hist(X_train.yr)
plt.show()
plt.hist(X_val.yr)
plt.show()
plt.hist(X_test.yr)
plt.hist(X_val.yr)
plt.hist(X_test.yr)
plt.show()
X_test.yr.min()
plt.hist(X_test.yr)
plt.hist(X_test['yr'])
X_test['yr']
plt.hist(X_test['yr'])
plt.show()
X_test
X_test.drop_duplicates()
X_test_final
X_test_final = X_test_regr[X_test_regr['class'] == '1'].drop(columns = ['class'])
# X_test_final = X_test_regr[X_test_regr['fat'] > 0].drop(columns = ['class'])
X_test_final = X_test_final.drop_duplicates()
X_test_final
X_test_regr
train_rm, X_test_regr, y_train_rm, y_test_regr = time_based_train_test_split(df.drop(columns = ['has_fat']), perc=0.95, pred_col = "fat")
X_test_regr
y_test_pred
len(y_test_pred)
y_test_pred = best_classifier.predict(X_test)
len(y_test_pred)
X_train, X_test, y_train, y_test = time_based_train_test_split(df.drop(columns = ['fat']), perc=0.95, pred_col = "has_fat")
X_test
y_test_pred = best_classifier.predict(X_test)
y_test_pred
len(y_test_pred)
X_test_regr
X_test_regr['class'] = y_test_pred
y_test_regr
X_test_regr['fat'] = y_test_regr
X_test_regr
X_test_final = X_test_regr[X_test_regr['class'] == '1'].drop(columns = ['class'])
X_test_final
X_test_final = X_test_final.drop_duplicates()
X_test_final
X_train, X_test, y_train, y_test = time_based_train_test_split(df = df_g, perc=0.95, year_col="yr", pred_col='fat')
X_train
X_train, X_test, y_train, y_test = time_based_train_test_split(df = df[df['fat'> 0]], perc=0.95, year_col="yr", pred_col='fat')
df[df['fat'> 0]]
df[df['fat']> 0]
X = df[df['fat'] > 0].drop(columns = ['has_fat', 'fat'])
y = df[df['fat'] > 0].has_fat
X_train, X_test, y_train, y_test = time_based_train_test_split(df = df[df['fat']> 0], perc=0.95, year_col="yr", pred_col='fat')
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
X_train
X_val
X_test = X_test_final.drop(columns = ['fat'])
X_train, X_test, y_train, y_test = time_based_train_test_split(df = df[df['fat']> 0], perc=0.95, year_col="yr", pred_col='fat')
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
X_test = X_test_final.drop(columns = ['fat'])
y_test = X_test_final['fat']
X_test
X_val
best_regressor = tune_regressor(X_train, y_train, X_val, y_val, model_file = model_file)
print(best_regressor)
# Calculate MSE for the training set
y_train_pred = best_regressor.predict(X_train)
# mse_train = mean_squared_error(np.expm1(y_train).astype(int), np.expm1(y_train_pred).astype(int)).round(3)
# r2_train = r2_score(np.expm1(y_train).astype(int), np.expm1(y_train_pred).astype(int)).round(3)
mse_train = mean_squared_error(y_train.astype(int), y_train_pred.astype(int)).round(3)
msle_train = mean_squared_log_error(y_train.astype(int), y_train_pred.astype(int)).round(3)
# pinball_train = d2_pinball_score(y_train.astype(int), y_train_pred.astype(int)).round(3)
mape_train = mean_absolute_percentage_error(y_train.astype(int), y_train_pred.astype(int)).round(3)
from sklearn.metrics import mean_absolute_percentage_error
mape_train = mean_absolute_percentage_error(y_train.astype(int), y_train_pred.astype(int)).round(3)
# Calculate MSE for the validation set
y_val_pred = best_regressor.predict(X_val)
# mse_val = mean_squared_error(np.expm1(y_val).astype(int), np.expm1(y_val_pred).astype(int)).round(3)
# r2_val = r2_score(np.expm1(y_val).astype(int), np.expm1(y_val_pred).astype(int)).round(3)
mse_val = mean_squared_error(y_val.astype(int), y_val_pred.astype(int)).round(3)
msle_val = mean_squared_log_error(y_val.astype(int), y_val_pred.astype(int)).round(3)
# pinball_val = d2_pinball_score(y_val.astype(int), y_val_pred.astype(int)).round(3)
mape_val = mean_absolute_percentage_error(y_val.astype(int), y_val_pred.astype(int)).round(3)
# Calculate MSE for the test set
y_test_pred = best_regressor.predict(X_test)
# mse_test = mean_squared_error(np.expm1(y_test).astype(int), np.expm1(y_test_pred).astype(int)).round(3)
# r2_test = r2_score(np.expm1(y_test).astype(int), np.expm1(y_test_pred).astype(int)).round(3)
y_test_pred = best_regressor.predict(X_test)
X_test
y_test
y = df[df['fat'] > 0].fat
y_val_pred
y_test_pred = best_regressor.predict(X_test)
X_train
X_test
X_train
X_val = X_val.drop(columns=['has_fat'])
best_regressor = tune_regressor(X_train, y_train, X_val, y_val, model_file = model_file)
print(best_regressor)
# Calculate MSE for the training set
y_train_pred = best_regressor.predict(X_train)
# mse_train = mean_squared_error(np.expm1(y_train).astype(int), np.expm1(y_train_pred).astype(int)).round(3)
# r2_train = r2_score(np.expm1(y_train).astype(int), np.expm1(y_train_pred).astype(int)).round(3)
mse_train = mean_squared_error(y_train.astype(int), y_train_pred.astype(int)).round(3)
msle_train = mean_squared_log_error(y_train.astype(int), y_train_pred.astype(int)).round(3)
# pinball_train = d2_pinball_score(y_train.astype(int), y_train_pred.astype(int)).round(3)
mape_train = mean_absolute_percentage_error(y_train.astype(int), y_train_pred.astype(int)).round(3)
# Calculate MSE for the validation set
y_val_pred = best_regressor.predict(X_val)
# mse_val = mean_squared_error(np.expm1(y_val).astype(int), np.expm1(y_val_pred).astype(int)).round(3)
# r2_val = r2_score(np.expm1(y_val).astype(int), np.expm1(y_val_pred).astype(int)).round(3)
mse_val = mean_squared_error(y_val.astype(int), y_val_pred.astype(int)).round(3)
msle_val = mean_squared_log_error(y_val.astype(int), y_val_pred.astype(int)).round(3)
# pinball_val = d2_pinball_score(y_val.astype(int), y_val_pred.astype(int)).round(3)
mape_val = mean_absolute_percentage_error(y_val.astype(int), y_val_pred.astype(int)).round(3)
# Calculate MSE for the test set
y_test_pred = best_regressor.predict(X_test)
# mse_test = mean_squared_error(np.expm1(y_test).astype(int), np.expm1(y_test_pred).astype(int)).round(3)
# r2_test = r2_score(np.expm1(y_test).astype(int), np.expm1(y_test_pred).astype(int)).round(3)
mse_test = mean_squared_error(y_test.astype(int), y_test_pred.astype(int)).round(3)
msle_test = mean_squared_log_error(y_test.astype(int), y_test_pred.astype(int)).round(3)
# pinball_test = d2_pinball_score(y_test.astype(int), y_test_pred.astype(int)).round(3)
mape_test = mean_absolute_percentage_error(y_test.astype(int), y_test_pred.astype(int)).round(3)
print(f"Training MSE: {mse_train}")
print(f"Validation MSE: {mse_val}")
print(f"Test MSE: {mse_test} \n")
print(f"Training MSLE: {msle_train}")
print(f"Validation  MSLE: {msle_val}")
print(f"Test  MSLE: {msle_test} \n")
print(f"Training MAPE: {mape_train}")
print(f"Validation  MAPE: {mape_val}")
print(f"Test  MAPE: {mape_test} \n")
# Regressor Feature Importance
# As the best model can differ, we use a try. Not always it will be simple to get the feature importance
try:
coefficients = best_regressor.feature_importances_
feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})
feature_importance = feature_importance.sort_values('Importance', ascending=False)
feature_importance.plot.bar(x='Feature', y='Importance', figsize=(12, 12))
plt.xticks(rotation=45, ha='right')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.savefig('output/regression-feat-importance.png')
plt.show()
except:
try:
coefficients = best_regressor.coef_
feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients.squeeze())})
feature_importance = feature_importance.sort_values('Importance', ascending=False)
feature_importance.plot.bar(x='Feature', y='Importance', figsize=(12, 12))
plt.xticks(rotation=45, ha='right')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.savefig('output/regression-feat-importance.png')
plt.show()
except:
print("No Feature Importance Possible for this Model")
sd = np.std(df_g.fat).round(1)
m = np.mean(df_g.fat).round(1)
plt_error_map(y_train, y_val, y_test, y_train_pred, y_val_pred, y_test_pred)
df_train = pd.DataFrame({'pred': y_train_pred, 'real': y_train})
df_train = df_train.groupby('real')['pred'].mean().reset_index()
df_val = pd.DataFrame({'pred': y_val_pred, 'real': y_val})
df_val = df_val.groupby('real')['pred'].mean().reset_index()
df_test = pd.DataFrame({'pred': y_test_pred, 'real': y_test})
df_test = df_test.groupby('real')['pred'].mean().reset_index()
plt_error_map(df_train.real, df_val.real, df_test.real, df_train.pred, df_val.pred, df_test.pred)
X_test_final
X_train.columns
plt.hist(df.has_fat)
plt.hist()
plt.show()
data_file = 'input/us_tornado_dataset_1950_2021.csv'
# Data wrangling ###############################################################
print("Wrangling the data... \n")
df = pd.read_csv(data_file)
# df.loc[df.mag==-9, 'mag'] = 0 #unknown magnitude when -9, so 0
df.loc[df.elat == 0.0, 'elat'] = df['slat'] #unknown final lat/long --> = to initial ones
df.loc[df.elon == 0.0, 'elon'] = df['slon']
df['traveled_d_km'] = df.apply(calculate_distance, axis=1)
df['len_km'] = df['len'].apply(miles_to_kilometers)
df['wid_km'] = df['wid'].apply(yards_to_kilometers)
df['area_sq_km'] = df.apply(calculate_tornado_area, axis=1)
# Format 'mo' and 'dy' columns with leading zeros
df['mo'] = df['mo'].apply(lambda x: str(x).zfill(2))
df['dy'] = df['dy'].apply(lambda x: str(x).zfill(2))
# Create a new 'date' column in the "year-month-day" format
df['date'] = pd.to_datetime(df[['yr', 'mo', 'dy']].astype(str).agg('-'.join, axis=1), format='%Y-%m-%d')
df['yr'] = df['yr'].astype(int)
df['mo'] = df['mo'].astype(int)
df['dy'] = df['dy'].astype(int)
# Extract the day of the week and create a new 'day_of_week' column
df['day_of_week'] = df['date'].dt.dayofweek
# Keeping only relevant columns
df = df[['yr', 'mo', 'dy', 'fat', 'traveled_d_km','area_sq_km', 'day_of_week']]
# Outlier Removal
df = df[df['fat'] < 30] # histogram based
df = df[df['traveled_d_km'] < 50] # histogram based
df = df[df['area_sq_km'] < 20] # histogram based
plt.hist(df.has_fat)
df.loc[:, 'has_fat'] = df['fat'].apply(transform_value)
df['has_fat'] = df['has_fat'].astype('category')
plt.hist(df.has_fat)
plt.show()
df = df[df['fat'] < 30] # histogram based
df = df[df['traveled_d_km'] < 50] # histogram based
df = df[df['area_sq_km'] < 20] # histogram based
# Filtering death
df_zero = df[df.fat==0]
# Filtering death
df_g = df[df.fat>0]
# replication factor to balance death
rep_f = int(len(df_zero) / len(df_g))
# oversample of death
df_gg = pd.concat([df_g] * rep_f, ignore_index=True)
# combining dfs again
frames = [df_zero, df_gg]
df = pd.concat(frames, ignore_index=True)
######## Classification ########################################################
print("Classifying... \n")
df.loc[:, 'has_fat'] = df['fat'].apply(transform_value)
df['has_fat'] = df['has_fat'].astype('category')
plt.hist(df.fat)
plt.show()
plt.hist(df.has_fat)
plt.show()
df.yr.min()
X_train.max()
X_train, X_test, y_train, y_test = time_based_train_test_split(df.drop(columns = ['fat']), perc=0.95, pred_col = "has_fat")
X_train.yr.max()
df.yr.max()
20/90
20/99
20*90
20*99
30/420
